package Project;

//import static com.googlecode.javacv.cpp.opencv_core.CV_AA;
//import static com.googlecode.javacv.cpp.opencv_core.cvAbsDiff;
//import static com.googlecode.javacv.cpp.opencv_core.cvClearMemStorage;
//import static com.googlecode.javacv.cpp.opencv_core.cvGetSeqElem;
//import static com.googlecode.javacv.cpp.opencv_core.cvLoad;
//import static com.googlecode.javacv.cpp.opencv_core.cvPoint;
//import static com.googlecode.javacv.cpp.opencv_core.cvRectangle;
import static com.googlecode.javacv.cpp.opencv_objdetect.CV_HAAR_DO_CANNY_PRUNING;
import static com.googlecode.javacv.cpp.opencv_objdetect.cvHaarDetectObjects;
import static com.googlecode.javacv.cpp.opencv_core.*;

import com.googlecode.javacv.cpp.opencv_core;
import com.googlecode.javacv.cpp.opencv_core.CvMat;
import com.googlecode.javacv.cpp.opencv_core.CvMemStorage;
import com.googlecode.javacv.cpp.opencv_core.CvRect;
import com.googlecode.javacv.cpp.opencv_core.CvScalar;
import com.googlecode.javacv.cpp.opencv_core.CvSeq;
import com.googlecode.javacv.cpp.opencv_core.IplImage;
import com.googlecode.javacv.cpp.opencv_objdetect.CvHaarClassifierCascade;

/**
 * Does the facial recognition
 * 
 * @author Marcus
 * 
 */
public class Analyzer {
	private CvHaarClassifierCascade faceCascade;
	private CvMemStorage storage;

	/**
	 * Constructor for the analyzer.
	 * 
	 * @throws ClassiferLoadFailure
	 */
	Analyzer() throws ClassiferLoadFailure {
		String classifierDir = Settings.CLASSIFIER_DIR
				+ "haarcascade_frontalface_default.xml";
		faceCascade = new CvHaarClassifierCascade(cvLoad(classifierDir));
		storage = CvMemStorage.create();
		if (faceCascade.isNull()) {
			throw new ClassiferLoadFailure(classifierDir);
		}
	}

	/**
	 * This will look for faces and return the coordinates of the faces
	 * 
	 * @param inputMat
	 *            The original unedited image
	 * @return The location of all the faces
	 */
	public CvSeq detectFaces(IplImage input) {
		CvSeq rects = cvHaarDetectObjects(input, faceCascade, storage, 1.5, 3,
				CV_HAAR_DO_CANNY_PRUNING);
		cvClearMemStorage(storage);
		return rects;

	}

	/**
	 * This will return the split video streams. It will take the original video
	 * stream and the location of the faces as input. It will return two video
	 * streams as a tuple. One will have the faces only the other will have
	 * everything else.
	 */
	public void separateStreams(CvMat inputMat, CvSeq faces) {

	}

	/**
	 * This function will write onto of the frames that have facial data.
	 * 
	 * @param inputMat
	 *            The Mat of that is going to be blacked out
	 * @param rect
	 *            The rectangle that will be drawn
	 */
	public void blackOutFaces(IplImage input, CvSeq rects) {
		int total_Faces = rects.total();
		for (int i = 0; i < total_Faces; i++) {
			CvRect r = new CvRect(cvGetSeqElem(rects, i));
			cvRectangle(input, cvPoint(r.x(), r.y()- (int) (r.height()*.25)),
					cvPoint(r.width() + r.x(), r.height() + r.y()),
					CvScalar.BLACK, opencv_core.CV_FILLED, CV_AA, 0);

		}

	}

	/**
	 * This function will do the foreground extraction. Having a standardized
	 * size may be useful for simplification.
	 */
	public IplImage extractForeground(IplImage input1, IplImage input2) {
		IplImage output = input1.clone();
		cvAbsDiff(input1, input2, output);
		return output;

	}

	/**
	 * This function takes in the two split video streams and recombines them in
	 * to one video stream.
	 */
	public void recombineVideo() {

	}

}
